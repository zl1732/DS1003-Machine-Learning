{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from pytorch example 'Word Level Language Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.py\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        # Add holdout mask token\n",
    "        self.dictionary.add_word('<MASK>')\n",
    "        #print('check <MASK>')\n",
    "        #print(self.dictionary.idx2word)\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Borwn corpus for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brown_corpus = data.Corpus('./data/brown/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<MASK>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify <MASK exists>\n",
    "brown_corpus.dictionary.idx2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict()\n",
    "args.cuda = False\n",
    "args.bptt=5\n",
    "args.bsz = 20\n",
    "args.winSize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)#(dimension,start,length)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    if args.cuda:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    #seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    seq_len = 2*args.winSize+1\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(torch.LongTensor(args.bsz))\n",
    "    target.data.copy_(data.data[args.winSize])\n",
    "    data.data[args.winSize]=0\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "# %load model.py\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)[-1]\n",
    "        decoded = self.decoder(output.view(output.size(0), output.size(1)))\n",
    "#         print('output')\n",
    "#         print(output.data.size())\n",
    "#         print('-'*89)\n",
    "#         print('decoded')\n",
    "#         print(decoded.data.size())\n",
    "#         print('-'*89)\n",
    "#         print('hidden')\n",
    "#         print(hidden)\n",
    "#         print('-'*89)\n",
    "        #return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "import model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\n",
    "parser.add_argument('--data', type=str, default='./data/penn',\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str,  default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--winSize', type=int,  default=3,\n",
    "                    help='Window_size')\n",
    "args = parser.parse_args()\n",
    "args.bptt=2*args.winSize+1\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    if args.cuda:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    #seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    seq_len = 2*args.winSize+1\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(torch.LongTensor(data.size(1)))\n",
    "    target.data.copy_(data.data[args.winSize])\n",
    "    data.data[args.winSize]=0\n",
    "    if args.cuda:\n",
    "        target = target.cuda()\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    print(data_source.size())\n",
    "    for i in range(0,data_source.size(0)-args.bptt,args.bptt):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        #total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(args.batch_size)\n",
    "    #print(train_data.size(0))\n",
    "    #for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
    "    for batch in range(0,train_data.size(0)-args.bptt,args.bptt):\n",
    "        data, targets = get_batch(train_data, batch)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        #loss = criterion(output.view(-1, ntokens), targets)\n",
    "#         print(output.size())\n",
    "#         print(targets.size())\n",
    "        loss = criterion(output.view(-1,ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "        total_loss += loss.data\n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            #print(data)\n",
    "            cur_loss = total_loss[0] / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, (train_data.size(0)-args.winSize), lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = args.lr\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(args.save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 33\n",
      "2 66\n",
      "3 99\n",
      "4 132\n",
      "5 165\n",
      "6 198\n",
      "7 231\n",
      "8 264\n",
      "9 297\n",
      "10 330\n",
      "11 363\n",
      "12 396\n",
      "13 429\n",
      "14 462\n",
      "15 495\n",
      "16 528\n",
      "17 561\n",
      "18 594\n",
      "19 627\n",
      "20 660\n",
      "21 693\n",
      "22 726\n",
      "23 759\n",
      "24 792\n",
      "25 825\n",
      "26 858\n",
      "27 891\n",
      "28 924\n",
      "29 957\n",
      "30 990\n",
      "31 1023\n",
      "32 1056\n",
      "33 1089\n",
      "34 1122\n",
      "35 1155\n",
      "36 1188\n",
      "37 1221\n",
      "38 1254\n",
      "39 1287\n",
      "40 1320\n",
      "41 1353\n",
      "42 1386\n",
      "43 1419\n",
      "44 1452\n",
      "45 1485\n",
      "46 1518\n",
      "47 1551\n",
      "48 1584\n",
      "49 1617\n",
      "50 1650\n",
      "51 1683\n",
      "52 1716\n",
      "53 1749\n",
      "54 1782\n",
      "55 1815\n",
      "56 1848\n",
      "57 1881\n",
      "58 1914\n",
      "59 1947\n",
      "60 1980\n",
      "61 2013\n",
      "62 2046\n",
      "63 2079\n",
      "64 2112\n",
      "65 2145\n",
      "66 2178\n",
      "67 2211\n",
      "68 2244\n",
      "69 2277\n",
      "70 2310\n",
      "71 2343\n",
      "72 2376\n",
      "73 2409\n",
      "74 2442\n",
      "75 2475\n",
      "76 2508\n",
      "77 2541\n",
      "78 2574\n",
      "79 2607\n",
      "80 2640\n",
      "81 2673\n",
      "82 2706\n",
      "83 2739\n",
      "84 2772\n",
      "85 2805\n",
      "86 2838\n",
      "87 2871\n",
      "88 2904\n",
      "89 2937\n",
      "90 2970\n",
      "91 3003\n",
      "92 3036\n",
      "93 3069\n",
      "94 3102\n",
      "95 3135\n",
      "96 3168\n",
      "97 3201\n",
      "98 3234\n",
      "99 3267\n",
      "100 3300\n",
      "101 3333\n",
      "102 3366\n",
      "103 3399\n",
      "104 3432\n",
      "105 3465\n",
      "106 3498\n",
      "107 3531\n",
      "108 3564\n",
      "109 3597\n",
      "110 3630\n",
      "111 3663\n",
      "112 3696\n",
      "113 3729\n",
      "114 3762\n",
      "115 3795\n",
      "116 3828\n",
      "117 3861\n",
      "118 3894\n",
      "119 3927\n",
      "120 3960\n",
      "121 3993\n",
      "122 4026\n",
      "123 4059\n",
      "124 4092\n",
      "125 4125\n",
      "126 4158\n",
      "127 4191\n",
      "128 4224\n",
      "129 4257\n",
      "130 4290\n",
      "131 4323\n",
      "132 4356\n",
      "133 4389\n",
      "134 4422\n",
      "135 4455\n",
      "136 4488\n",
      "137 4521\n",
      "138 4554\n",
      "139 4587\n",
      "140 4620\n",
      "141 4653\n",
      "142 4686\n",
      "143 4719\n",
      "144 4752\n",
      "145 4785\n",
      "146 4818\n",
      "147 4851\n",
      "148 4884\n",
      "149 4917\n",
      "150 4950\n",
      "151 4983\n",
      "152 5016\n",
      "153 5049\n",
      "154 5082\n",
      "155 5115\n",
      "156 5148\n",
      "157 5181\n",
      "158 5214\n",
      "159 5247\n",
      "160 5280\n",
      "161 5313\n",
      "162 5346\n",
      "163 5379\n",
      "164 5412\n",
      "165 5445\n",
      "166 5478\n",
      "167 5511\n",
      "168 5544\n",
      "169 5577\n",
      "170 5610\n",
      "171 5643\n",
      "172 5676\n",
      "173 5709\n",
      "174 5742\n",
      "175 5775\n",
      "176 5808\n",
      "177 5841\n",
      "178 5874\n",
      "179 5907\n",
      "180 5940\n",
      "181 5973\n",
      "182 6006\n",
      "183 6039\n",
      "184 6072\n",
      "185 6105\n",
      "186 6138\n",
      "187 6171\n",
      "188 6204\n",
      "189 6237\n",
      "190 6270\n",
      "191 6303\n",
      "192 6336\n",
      "193 6369\n",
      "194 6402\n",
      "195 6435\n",
      "196 6468\n",
      "197 6501\n",
      "198 6534\n",
      "199 6567\n",
      "200 6600\n",
      "201 6633\n",
      "202 6666\n",
      "203 6699\n",
      "204 6732\n",
      "205 6765\n",
      "206 6798\n",
      "207 6831\n",
      "208 6864\n",
      "209 6897\n",
      "210 6930\n",
      "211 6963\n",
      "212 6996\n",
      "213 7029\n",
      "214 7062\n",
      "215 7095\n",
      "216 7128\n",
      "217 7161\n",
      "218 7194\n",
      "219 7227\n",
      "220 7260\n",
      "221 7293\n",
      "222 7326\n",
      "223 7359\n",
      "224 7392\n",
      "225 7425\n",
      "226 7458\n",
      "227 7491\n",
      "228 7524\n",
      "229 7557\n",
      "230 7590\n",
      "231 7623\n",
      "232 7656\n",
      "233 7689\n",
      "234 7722\n",
      "235 7755\n",
      "236 7788\n",
      "237 7821\n",
      "238 7854\n",
      "239 7887\n",
      "240 7920\n",
      "241 7953\n",
      "242 7986\n",
      "243 8019\n",
      "244 8052\n",
      "245 8085\n",
      "246 8118\n",
      "247 8151\n",
      "248 8184\n",
      "249 8217\n",
      "250 8250\n",
      "251 8283\n",
      "252 8316\n",
      "253 8349\n",
      "254 8382\n",
      "255 8415\n",
      "256 8448\n",
      "257 8481\n",
      "258 8514\n",
      "259 8547\n",
      "260 8580\n",
      "261 8613\n",
      "262 8646\n",
      "263 8679\n",
      "264 8712\n",
      "265 8745\n",
      "266 8778\n",
      "267 8811\n",
      "268 8844\n",
      "269 8877\n",
      "270 8910\n",
      "271 8943\n",
      "272 8976\n",
      "273 9009\n",
      "274 9042\n",
      "275 9075\n",
      "276 9108\n",
      "277 9141\n",
      "278 9174\n",
      "279 9207\n",
      "280 9240\n",
      "281 9273\n",
      "282 9306\n",
      "283 9339\n",
      "284 9372\n",
      "285 9405\n",
      "286 9438\n",
      "287 9471\n",
      "288 9504\n",
      "289 9537\n",
      "290 9570\n",
      "291 9603\n",
      "292 9636\n",
      "293 9669\n",
      "294 9702\n",
      "295 9735\n",
      "296 9768\n",
      "297 9801\n",
      "298 9834\n",
      "299 9867\n",
      "300 9900\n",
      "301 9933\n",
      "302 9966\n",
      "303 9999\n",
      "304 10032\n",
      "305 10065\n",
      "306 10098\n",
      "307 10131\n",
      "308 10164\n",
      "309 10197\n",
      "310 10230\n",
      "311 10263\n",
      "312 10296\n",
      "313 10329\n",
      "314 10362\n",
      "315 10395\n",
      "316 10428\n",
      "317 10461\n",
      "318 10494\n",
      "319 10527\n",
      "320 10560\n",
      "321 10593\n",
      "322 10626\n",
      "323 10659\n",
      "324 10692\n",
      "325 10725\n",
      "326 10758\n",
      "327 10791\n",
      "328 10824\n",
      "329 10857\n",
      "330 10890\n",
      "331 10923\n",
      "332 10956\n",
      "333 10989\n",
      "334 11022\n",
      "335 11055\n",
      "336 11088\n",
      "337 11121\n",
      "338 11154\n",
      "339 11187\n",
      "340 11220\n",
      "341 11253\n",
      "342 11286\n",
      "343 11319\n",
      "344 11352\n",
      "345 11385\n",
      "346 11418\n",
      "347 11451\n",
      "348 11484\n",
      "349 11517\n",
      "350 11550\n",
      "351 11583\n",
      "352 11616\n",
      "353 11649\n",
      "354 11682\n",
      "355 11715\n",
      "356 11748\n",
      "357 11781\n",
      "358 11814\n",
      "359 11847\n",
      "360 11880\n",
      "361 11913\n",
      "362 11946\n",
      "363 11979\n",
      "364 12012\n",
      "365 12045\n",
      "366 12078\n",
      "367 12111\n",
      "368 12144\n",
      "369 12177\n",
      "370 12210\n",
      "371 12243\n",
      "372 12276\n",
      "373 12309\n",
      "374 12342\n",
      "375 12375\n",
      "376 12408\n",
      "377 12441\n",
      "378 12474\n",
      "379 12507\n",
      "380 12540\n",
      "381 12573\n",
      "382 12606\n",
      "383 12639\n",
      "384 12672\n",
      "385 12705\n",
      "386 12738\n",
      "387 12771\n",
      "388 12804\n",
      "389 12837\n",
      "390 12870\n",
      "391 12903\n",
      "392 12936\n",
      "393 12969\n",
      "394 13002\n",
      "395 13035\n",
      "396 13068\n",
      "397 13101\n",
      "398 13134\n",
      "399 13167\n",
      "400 13200\n",
      "401 13233\n",
      "402 13266\n",
      "403 13299\n",
      "404 13332\n",
      "405 13365\n",
      "406 13398\n",
      "407 13431\n",
      "408 13464\n",
      "409 13497\n",
      "410 13530\n",
      "411 13563\n",
      "412 13596\n",
      "413 13629\n",
      "414 13662\n",
      "415 13695\n",
      "416 13728\n",
      "417 13761\n",
      "418 13794\n",
      "419 13827\n",
      "420 13860\n",
      "421 13893\n",
      "422 13926\n",
      "423 13959\n",
      "424 13992\n",
      "425 14025\n",
      "426 14058\n",
      "427 14091\n",
      "428 14124\n",
      "429 14157\n",
      "430 14190\n",
      "431 14223\n",
      "432 14256\n",
      "433 14289\n",
      "434 14322\n",
      "435 14355\n",
      "436 14388\n",
      "437 14421\n",
      "438 14454\n",
      "439 14487\n",
      "440 14520\n",
      "441 14553\n",
      "442 14586\n",
      "443 14619\n",
      "444 14652\n",
      "445 14685\n",
      "446 14718\n",
      "447 14751\n",
      "448 14784\n",
      "449 14817\n",
      "450 14850\n",
      "451 14883\n",
      "452 14916\n",
      "453 14949\n",
      "454 14982\n",
      "455 15015\n",
      "456 15048\n",
      "457 15081\n",
      "458 15114\n",
      "459 15147\n",
      "460 15180\n",
      "461 15213\n",
      "462 15246\n",
      "463 15279\n",
      "464 15312\n",
      "465 15345\n",
      "466 15378\n",
      "467 15411\n",
      "468 15444\n",
      "469 15477\n",
      "470 15510\n",
      "471 15543\n",
      "472 15576\n",
      "473 15609\n",
      "474 15642\n",
      "475 15675\n",
      "476 15708\n",
      "477 15741\n",
      "478 15774\n",
      "479 15807\n",
      "480 15840\n",
      "481 15873\n",
      "482 15906\n",
      "483 15939\n",
      "484 15972\n",
      "485 16005\n",
      "486 16038\n",
      "487 16071\n",
      "488 16104\n",
      "489 16137\n",
      "490 16170\n",
      "491 16203\n",
      "492 16236\n",
      "493 16269\n",
      "494 16302\n",
      "495 16335\n",
      "496 16368\n",
      "497 16401\n",
      "498 16434\n",
      "499 16467\n",
      "500 16500\n",
      "501 16533\n",
      "502 16566\n",
      "503 16599\n",
      "504 16632\n",
      "505 16665\n",
      "506 16698\n",
      "507 16731\n",
      "508 16764\n",
      "509 16797\n",
      "510 16830\n",
      "511 16863\n",
      "512 16896\n",
      "513 16929\n",
      "514 16962\n",
      "515 16995\n",
      "516 17028\n",
      "517 17061\n",
      "518 17094\n",
      "519 17127\n",
      "520 17160\n",
      "521 17193\n",
      "522 17226\n",
      "523 17259\n",
      "524 17292\n",
      "525 17325\n",
      "526 17358\n",
      "527 17391\n",
      "528 17424\n",
      "529 17457\n",
      "530 17490\n",
      "531 17523\n",
      "532 17556\n",
      "533 17589\n",
      "534 17622\n",
      "535 17655\n",
      "536 17688\n",
      "537 17721\n",
      "538 17754\n",
      "539 17787\n",
      "540 17820\n",
      "541 17853\n",
      "542 17886\n",
      "543 17919\n",
      "544 17952\n",
      "545 17985\n",
      "546 18018\n",
      "547 18051\n",
      "548 18084\n",
      "549 18117\n",
      "550 18150\n",
      "551 18183\n",
      "552 18216\n",
      "553 18249\n",
      "554 18282\n",
      "555 18315\n",
      "556 18348\n",
      "557 18381\n",
      "558 18414\n",
      "559 18447\n",
      "560 18480\n",
      "561 18513\n",
      "562 18546\n",
      "563 18579\n",
      "564 18612\n",
      "565 18645\n",
      "566 18678\n",
      "567 18711\n",
      "568 18744\n",
      "569 18777\n",
      "570 18810\n",
      "571 18843\n",
      "572 18876\n",
      "573 18909\n",
      "574 18942\n",
      "575 18975\n",
      "576 19008\n",
      "577 19041\n",
      "578 19074\n",
      "579 19107\n",
      "580 19140\n",
      "581 19173\n",
      "582 19206\n",
      "583 19239\n",
      "584 19272\n",
      "585 19305\n",
      "586 19338\n",
      "587 19371\n",
      "588 19404\n",
      "589 19437\n",
      "590 19470\n",
      "591 19503\n",
      "592 19536\n",
      "593 19569\n",
      "594 19602\n",
      "595 19635\n",
      "596 19668\n",
      "597 19701\n",
      "598 19734\n",
      "599 19767\n",
      "600 19800\n",
      "601 19833\n",
      "602 19866\n",
      "603 19899\n",
      "604 19932\n",
      "605 19965\n",
      "606 19998\n",
      "607 20031\n",
      "608 20064\n",
      "609 20097\n",
      "610 20130\n",
      "611 20163\n",
      "612 20196\n",
      "613 20229\n",
      "614 20262\n",
      "615 20295\n",
      "616 20328\n",
      "617 20361\n",
      "618 20394\n",
      "619 20427\n",
      "620 20460\n",
      "621 20493\n",
      "622 20526\n",
      "623 20559\n",
      "624 20592\n",
      "625 20625\n",
      "626 20658\n",
      "627 20691\n",
      "628 20724\n",
      "629 20757\n",
      "630 20790\n",
      "631 20823\n",
      "632 20856\n",
      "633 20889\n",
      "634 20922\n",
      "635 20955\n",
      "636 20988\n",
      "637 21021\n",
      "638 21054\n",
      "639 21087\n",
      "640 21120\n",
      "641 21153\n",
      "642 21186\n",
      "643 21219\n",
      "644 21252\n",
      "645 21285\n",
      "646 21318\n",
      "647 21351\n",
      "648 21384\n",
      "649 21417\n",
      "650 21450\n",
      "651 21483\n",
      "652 21516\n",
      "653 21549\n",
      "654 21582\n",
      "655 21615\n",
      "656 21648\n",
      "657 21681\n",
      "658 21714\n",
      "659 21747\n",
      "660 21780\n",
      "661 21813\n",
      "662 21846\n",
      "663 21879\n",
      "664 21912\n",
      "665 21945\n",
      "666 21978\n",
      "667 22011\n",
      "668 22044\n",
      "669 22077\n",
      "670 22110\n",
      "671 22143\n",
      "672 22176\n",
      "673 22209\n",
      "674 22242\n",
      "675 22275\n",
      "676 22308\n",
      "677 22341\n",
      "678 22374\n",
      "679 22407\n",
      "680 22440\n",
      "681 22473\n",
      "682 22506\n",
      "683 22539\n",
      "684 22572\n",
      "685 22605\n",
      "686 22638\n",
      "687 22671\n",
      "688 22704\n",
      "689 22737\n",
      "690 22770\n",
      "691 22803\n",
      "692 22836\n",
      "693 22869\n",
      "694 22902\n",
      "695 22935\n",
      "696 22968\n",
      "697 23001\n",
      "698 23034\n",
      "699 23067\n",
      "700 23100\n",
      "701 23133\n",
      "702 23166\n",
      "703 23199\n",
      "704 23232\n",
      "705 23265\n",
      "706 23298\n",
      "707 23331\n",
      "708 23364\n",
      "709 23397\n",
      "710 23430\n",
      "711 23463\n",
      "712 23496\n",
      "713 23529\n",
      "714 23562\n",
      "715 23595\n",
      "716 23628\n",
      "717 23661\n",
      "718 23694\n",
      "719 23727\n",
      "720 23760\n",
      "721 23793\n",
      "722 23826\n",
      "723 23859\n",
      "724 23892\n",
      "725 23925\n",
      "726 23958\n",
      "727 23991\n",
      "728 24024\n",
      "729 24057\n",
      "730 24090\n",
      "731 24123\n",
      "732 24156\n",
      "733 24189\n",
      "734 24222\n",
      "735 24255\n",
      "736 24288\n",
      "737 24321\n",
      "738 24354\n",
      "739 24387\n",
      "740 24420\n",
      "741 24453\n",
      "742 24486\n",
      "743 24519\n",
      "744 24552\n",
      "745 24585\n",
      "746 24618\n",
      "747 24651\n",
      "748 24684\n",
      "749 24717\n",
      "750 24750\n",
      "751 24783\n",
      "752 24816\n",
      "753 24849\n",
      "754 24882\n",
      "755 24915\n",
      "756 24948\n",
      "757 24981\n",
      "758 25014\n",
      "759 25047\n",
      "760 25080\n",
      "761 25113\n",
      "762 25146\n",
      "763 25179\n",
      "764 25212\n",
      "765 25245\n",
      "766 25278\n",
      "767 25311\n",
      "768 25344\n",
      "769 25377\n",
      "770 25410\n",
      "771 25443\n",
      "772 25476\n",
      "773 25509\n",
      "774 25542\n",
      "775 25575\n",
      "776 25608\n",
      "777 25641\n",
      "778 25674\n",
      "779 25707\n",
      "780 25740\n",
      "781 25773\n",
      "782 25806\n",
      "783 25839\n",
      "784 25872\n",
      "785 25905\n",
      "786 25938\n",
      "787 25971\n",
      "788 26004\n",
      "789 26037\n",
      "790 26070\n",
      "791 26103\n",
      "792 26136\n",
      "793 26169\n",
      "794 26202\n",
      "795 26235\n",
      "796 26268\n",
      "797 26301\n",
      "798 26334\n",
      "799 26367\n",
      "800 26400\n",
      "801 26433\n",
      "802 26466\n",
      "803 26499\n",
      "804 26532\n",
      "805 26565\n",
      "806 26598\n",
      "807 26631\n",
      "808 26664\n",
      "809 26697\n",
      "810 26730\n",
      "811 26763\n",
      "812 26796\n",
      "813 26829\n",
      "814 26862\n",
      "815 26895\n",
      "816 26928\n",
      "817 26961\n",
      "818 26994\n",
      "819 27027\n",
      "820 27060\n",
      "821 27093\n",
      "822 27126\n",
      "823 27159\n",
      "824 27192\n",
      "825 27225\n",
      "826 27258\n",
      "827 27291\n",
      "828 27324\n",
      "829 27357\n",
      "830 27390\n",
      "831 27423\n",
      "832 27456\n",
      "833 27489\n",
      "834 27522\n",
      "835 27555\n",
      "836 27588\n",
      "837 27621\n",
      "838 27654\n",
      "839 27687\n",
      "840 27720\n",
      "841 27753\n",
      "842 27786\n",
      "843 27819\n",
      "844 27852\n",
      "845 27885\n",
      "846 27918\n",
      "847 27951\n",
      "848 27984\n",
      "849 28017\n",
      "850 28050\n",
      "851 28083\n",
      "852 28116\n",
      "853 28149\n",
      "854 28182\n",
      "855 28215\n",
      "856 28248\n",
      "857 28281\n",
      "858 28314\n",
      "859 28347\n",
      "860 28380\n",
      "861 28413\n",
      "862 28446\n",
      "863 28479\n",
      "864 28512\n",
      "865 28545\n",
      "866 28578\n",
      "867 28611\n",
      "868 28644\n",
      "869 28677\n",
      "870 28710\n",
      "871 28743\n",
      "872 28776\n",
      "873 28809\n",
      "874 28842\n",
      "875 28875\n",
      "876 28908\n",
      "877 28941\n",
      "878 28974\n",
      "879 29007\n",
      "880 29040\n",
      "881 29073\n",
      "882 29106\n",
      "883 29139\n",
      "884 29172\n",
      "885 29205\n",
      "886 29238\n",
      "887 29271\n",
      "888 29304\n",
      "889 29337\n",
      "890 29370\n",
      "891 29403\n",
      "892 29436\n",
      "893 29469\n",
      "894 29502\n",
      "895 29535\n",
      "896 29568\n",
      "897 29601\n",
      "898 29634\n",
      "899 29667\n",
      "900 29700\n",
      "901 29733\n",
      "902 29766\n",
      "903 29799\n",
      "904 29832\n",
      "905 29865\n",
      "906 29898\n",
      "907 29931\n",
      "908 29964\n",
      "909 29997\n",
      "910 30030\n",
      "911 30063\n",
      "912 30096\n",
      "913 30129\n",
      "914 30162\n",
      "915 30195\n",
      "916 30228\n",
      "917 30261\n",
      "918 30294\n",
      "919 30327\n",
      "920 30360\n",
      "921 30393\n",
      "922 30426\n",
      "923 30459\n",
      "924 30492\n",
      "925 30525\n",
      "926 30558\n",
      "927 30591\n",
      "928 30624\n",
      "929 30657\n",
      "930 30690\n",
      "931 30723\n",
      "932 30756\n",
      "933 30789\n",
      "934 30822\n",
      "935 30855\n",
      "936 30888\n",
      "937 30921\n",
      "938 30954\n",
      "939 30987\n",
      "940 31020\n",
      "941 31053\n",
      "942 31086\n",
      "943 31119\n",
      "944 31152\n",
      "945 31185\n",
      "946 31218\n",
      "947 31251\n",
      "948 31284\n",
      "949 31317\n",
      "950 31350\n",
      "951 31383\n",
      "952 31416\n",
      "953 31449\n",
      "954 31482\n",
      "955 31515\n",
      "956 31548\n",
      "957 31581\n",
      "958 31614\n",
      "959 31647\n",
      "960 31680\n",
      "961 31713\n",
      "962 31746\n",
      "963 31779\n",
      "964 31812\n",
      "965 31845\n",
      "966 31878\n",
      "967 31911\n",
      "968 31944\n",
      "969 31977\n",
      "970 32010\n",
      "971 32043\n",
      "972 32076\n",
      "973 32109\n",
      "974 32142\n",
      "975 32175\n",
      "976 32208\n",
      "977 32241\n",
      "978 32274\n",
      "979 32307\n",
      "980 32340\n",
      "981 32373\n",
      "982 32406\n",
      "983 32439\n",
      "984 32472\n",
      "985 32505\n",
      "986 32538\n",
      "987 32571\n",
      "988 32604\n",
      "989 32637\n",
      "990 32670\n",
      "991 32703\n",
      "992 32736\n",
      "993 32769\n",
      "994 32802\n",
      "995 32835\n",
      "996 32868\n",
      "997 32901\n",
      "998 32934\n",
      "999 32967\n",
      "1000 33000\n",
      "1001 33033\n",
      "1002 33066\n",
      "1003 33099\n",
      "1004 33132\n",
      "1005 33165\n",
      "1006 33198\n",
      "1007 33231\n",
      "1008 33264\n",
      "1009 33297\n",
      "1010 33330\n",
      "1011 33363\n",
      "1012 33396\n",
      "1013 33429\n",
      "1014 33462\n",
      "1015 33495\n",
      "1016 33528\n",
      "1017 33561\n",
      "1018 33594\n",
      "1019 33627\n",
      "1020 33660\n",
      "1021 33693\n",
      "1022 33726\n",
      "1023 33759\n",
      "1024 33792\n",
      "1025 33825\n",
      "1026 33858\n",
      "1027 33891\n",
      "1028 33924\n",
      "1029 33957\n",
      "1030 33990\n",
      "1031 34023\n",
      "1032 34056\n",
      "1033 34089\n",
      "1034 34122\n",
      "1035 34155\n",
      "1036 34188\n",
      "1037 34221\n",
      "1038 34254\n",
      "1039 34287\n",
      "1040 34320\n",
      "1041 34353\n",
      "1042 34386\n",
      "1043 34419\n",
      "1044 34452\n",
      "1045 34485\n",
      "1046 34518\n",
      "1047 34551\n",
      "1048 34584\n",
      "1049 34617\n",
      "1050 34650\n",
      "1051 34683\n",
      "1052 34716\n",
      "1053 34749\n",
      "1054 34782\n",
      "1055 34815\n",
      "1056 34848\n",
      "1057 34881\n",
      "1058 34914\n",
      "1059 34947\n",
      "1060 34980\n",
      "1061 35013\n",
      "1062 35046\n",
      "1063 35079\n",
      "1064 35112\n",
      "1065 35145\n",
      "1066 35178\n",
      "1067 35211\n",
      "1068 35244\n",
      "1069 35277\n",
      "1070 35310\n",
      "1071 35343\n",
      "1072 35376\n",
      "1073 35409\n",
      "1074 35442\n",
      "1075 35475\n",
      "1076 35508\n",
      "1077 35541\n",
      "1078 35574\n",
      "1079 35607\n",
      "1080 35640\n",
      "1081 35673\n",
      "1082 35706\n",
      "1083 35739\n",
      "1084 35772\n",
      "1085 35805\n",
      "1086 35838\n",
      "1087 35871\n",
      "1088 35904\n",
      "1089 35937\n",
      "1090 35970\n",
      "1091 36003\n",
      "1092 36036\n",
      "1093 36069\n",
      "1094 36102\n",
      "1095 36135\n",
      "1096 36168\n",
      "1097 36201\n",
      "1098 36234\n",
      "1099 36267\n",
      "1100 36300\n",
      "1101 36333\n",
      "1102 36366\n",
      "1103 36399\n",
      "1104 36432\n",
      "1105 36465\n",
      "1106 36498\n",
      "1107 36531\n",
      "1108 36564\n",
      "1109 36597\n",
      "1110 36630\n",
      "1111 36663\n",
      "1112 36696\n",
      "1113 36729\n",
      "1114 36762\n",
      "1115 36795\n",
      "1116 36828\n",
      "1117 36861\n",
      "1118 36894\n",
      "1119 36927\n",
      "1120 36960\n",
      "1121 36993\n",
      "1122 37026\n",
      "1123 37059\n",
      "1124 37092\n",
      "1125 37125\n",
      "1126 37158\n",
      "1127 37191\n",
      "1128 37224\n",
      "1129 37257\n",
      "1130 37290\n",
      "1131 37323\n",
      "1132 37356\n",
      "1133 37389\n",
      "1134 37422\n",
      "1135 37455\n",
      "1136 37488\n",
      "1137 37521\n",
      "1138 37554\n",
      "1139 37587\n",
      "1140 37620\n",
      "1141 37653\n",
      "1142 37686\n",
      "1143 37719\n",
      "1144 37752\n",
      "1145 37785\n",
      "1146 37818\n",
      "1147 37851\n",
      "1148 37884\n",
      "1149 37917\n",
      "1150 37950\n",
      "1151 37983\n",
      "1152 38016\n",
      "1153 38049\n",
      "1154 38082\n",
      "1155 38115\n",
      "1156 38148\n",
      "1157 38181\n",
      "1158 38214\n",
      "1159 38247\n",
      "1160 38280\n",
      "1161 38313\n",
      "1162 38346\n",
      "1163 38379\n",
      "1164 38412\n",
      "1165 38445\n",
      "1166 38478\n",
      "1167 38511\n",
      "1168 38544\n",
      "1169 38577\n",
      "1170 38610\n",
      "1171 38643\n",
      "1172 38676\n",
      "1173 38709\n",
      "1174 38742\n",
      "1175 38775\n",
      "1176 38808\n",
      "1177 38841\n",
      "1178 38874\n",
      "1179 38907\n",
      "1180 38940\n",
      "1181 38973\n",
      "1182 39006\n",
      "1183 39039\n",
      "1184 39072\n",
      "1185 39105\n",
      "1186 39138\n",
      "1187 39171\n",
      "1188 39204\n",
      "1189 39237\n",
      "1190 39270\n",
      "1191 39303\n",
      "1192 39336\n",
      "1193 39369\n",
      "1194 39402\n",
      "1195 39435\n",
      "1196 39468\n",
      "1197 39501\n",
      "1198 39534\n",
      "1199 39567\n",
      "1200 39600\n",
      "1201 39633\n",
      "1202 39666\n",
      "1203 39699\n",
      "1204 39732\n",
      "1205 39765\n",
      "1206 39798\n",
      "1207 39831\n",
      "1208 39864\n",
      "1209 39897\n",
      "1210 39930\n",
      "1211 39963\n",
      "1212 39996\n",
      "1213 40029\n",
      "1214 40062\n",
      "1215 40095\n",
      "1216 40128\n",
      "1217 40161\n",
      "1218 40194\n",
      "1219 40227\n",
      "1220 40260\n",
      "1221 40293\n",
      "1222 40326\n",
      "1223 40359\n",
      "1224 40392\n",
      "1225 40425\n",
      "1226 40458\n",
      "1227 40491\n",
      "1228 40524\n",
      "1229 40557\n",
      "1230 40590\n",
      "1231 40623\n",
      "1232 40656\n",
      "1233 40689\n",
      "1234 40722\n",
      "1235 40755\n",
      "1236 40788\n",
      "1237 40821\n",
      "1238 40854\n",
      "1239 40887\n",
      "1240 40920\n",
      "1241 40953\n",
      "1242 40986\n",
      "1243 41019\n",
      "1244 41052\n",
      "1245 41085\n",
      "1246 41118\n",
      "1247 41151\n",
      "1248 41184\n",
      "1249 41217\n",
      "1250 41250\n",
      "1251 41283\n",
      "1252 41316\n",
      "1253 41349\n",
      "1254 41382\n",
      "1255 41415\n",
      "1256 41448\n",
      "1257 41481\n",
      "1258 41514\n",
      "1259 41547\n",
      "1260 41580\n",
      "1261 41613\n",
      "1262 41646\n",
      "1263 41679\n",
      "1264 41712\n",
      "1265 41745\n",
      "1266 41778\n",
      "1267 41811\n",
      "1268 41844\n",
      "1269 41877\n",
      "1270 41910\n",
      "1271 41943\n",
      "1272 41976\n",
      "1273 42009\n",
      "1274 42042\n",
      "1275 42075\n",
      "1276 42108\n",
      "1277 42141\n",
      "1278 42174\n",
      "1279 42207\n",
      "1280 42240\n",
      "1281 42273\n",
      "1282 42306\n",
      "1283 42339\n",
      "1284 42372\n",
      "1285 42405\n",
      "1286 42438\n",
      "1287 42471\n",
      "1288 42504\n",
      "1289 42537\n",
      "1290 42570\n",
      "1291 42603\n",
      "1292 42636\n",
      "1293 42669\n",
      "1294 42702\n",
      "1295 42735\n",
      "1296 42768\n",
      "1297 42801\n",
      "1298 42834\n",
      "1299 42867\n",
      "1300 42900\n",
      "1301 42933\n",
      "1302 42966\n",
      "1303 42999\n",
      "1304 43032\n",
      "1305 43065\n",
      "1306 43098\n",
      "1307 43131\n",
      "1308 43164\n",
      "1309 43197\n",
      "1310 43230\n",
      "1311 43263\n",
      "1312 43296\n",
      "1313 43329\n",
      "1314 43362\n",
      "1315 43395\n",
      "1316 43428\n",
      "1317 43461\n",
      "1318 43494\n",
      "1319 43527\n",
      "1320 43560\n",
      "1321 43593\n",
      "1322 43626\n",
      "1323 43659\n",
      "1324 43692\n",
      "1325 43725\n",
      "1326 43758\n",
      "1327 43791\n",
      "1328 43824\n",
      "1329 43857\n",
      "1330 43890\n",
      "1331 43923\n",
      "1332 43956\n",
      "1333 43989\n",
      "1334 44022\n",
      "1335 44055\n",
      "1336 44088\n",
      "1337 44121\n",
      "1338 44154\n",
      "1339 44187\n",
      "1340 44220\n",
      "1341 44253\n",
      "1342 44286\n",
      "1343 44319\n",
      "1344 44352\n",
      "1345 44385\n",
      "1346 44418\n",
      "1347 44451\n",
      "1348 44484\n",
      "1349 44517\n",
      "1350 44550\n",
      "1351 44583\n",
      "1352 44616\n",
      "1353 44649\n",
      "1354 44682\n",
      "1355 44715\n",
      "1356 44748\n",
      "1357 44781\n",
      "1358 44814\n",
      "1359 44847\n",
      "1360 44880\n",
      "1361 44913\n",
      "1362 44946\n",
      "1363 44979\n",
      "1364 45012\n",
      "1365 45045\n",
      "1366 45078\n",
      "1367 45111\n",
      "1368 45144\n",
      "1369 45177\n",
      "1370 45210\n",
      "1371 45243\n",
      "1372 45276\n",
      "1373 45309\n",
      "1374 45342\n",
      "1375 45375\n",
      "1376 45408\n",
      "1377 45441\n",
      "1378 45474\n",
      "1379 45507\n",
      "1380 45540\n",
      "1381 45573\n",
      "1382 45606\n",
      "1383 45639\n",
      "1384 45672\n",
      "1385 45705\n",
      "1386 45738\n",
      "1387 45771\n",
      "1388 45804\n",
      "1389 45837\n",
      "1390 45870\n",
      "1391 45903\n",
      "1392 45936\n",
      "1393 45969\n",
      "1394 46002\n",
      "1395 46035\n",
      "1396 46068\n",
      "1397 46101\n",
      "1398 46134\n",
      "1399 46167\n",
      "1400 46200\n",
      "1401 46233\n",
      "1402 46266\n",
      "1403 46299\n",
      "1404 46332\n",
      "1405 46365\n",
      "1406 46398\n",
      "1407 46431\n",
      "1408 46464\n",
      "1409 46497\n",
      "1410 46530\n",
      "1411 46563\n",
      "1412 46596\n",
      "1413 46629\n",
      "1414 46662\n",
      "1415 46695\n",
      "1416 46728\n",
      "1417 46761\n",
      "1418 46794\n",
      "1419 46827\n",
      "1420 46860\n",
      "1421 46893\n",
      "1422 46926\n",
      "1423 46959\n",
      "1424 46992\n",
      "1425 47025\n",
      "1426 47058\n",
      "1427 47091\n",
      "1428 47124\n",
      "1429 47157\n",
      "1430 47190\n",
      "1431 47223\n",
      "1432 47256\n",
      "1433 47289\n",
      "1434 47322\n",
      "1435 47355\n",
      "1436 47388\n",
      "1437 47421\n",
      "1438 47454\n",
      "1439 47487\n",
      "1440 47520\n",
      "1441 47553\n",
      "1442 47586\n",
      "1443 47619\n",
      "1444 47652\n",
      "1445 47685\n",
      "1446 47718\n",
      "1447 47751\n",
      "1448 47784\n",
      "1449 47817\n",
      "1450 47850\n",
      "1451 47883\n",
      "1452 47916\n",
      "1453 47949\n",
      "1454 47982\n",
      "1455 48015\n",
      "1456 48048\n",
      "1457 48081\n",
      "1458 48114\n",
      "1459 48147\n",
      "1460 48180\n",
      "1461 48213\n",
      "1462 48246\n",
      "1463 48279\n",
      "1464 48312\n",
      "1465 48345\n",
      "1466 48378\n",
      "1467 48411\n",
      "1468 48444\n",
      "1469 48477\n",
      "1470 48510\n",
      "1471 48543\n",
      "1472 48576\n",
      "1473 48609\n",
      "1474 48642\n",
      "1475 48675\n",
      "1476 48708\n",
      "1477 48741\n",
      "1478 48774\n",
      "1479 48807\n",
      "1480 48840\n",
      "1481 48873\n",
      "1482 48906\n",
      "1483 48939\n",
      "1484 48972\n",
      "1485 49005\n",
      "1486 49038\n",
      "1487 49071\n",
      "1488 49104\n",
      "1489 49137\n",
      "1490 49170\n",
      "1491 49203\n",
      "1492 49236\n",
      "1493 49269\n",
      "1494 49302\n",
      "1495 49335\n",
      "1496 49368\n",
      "1497 49401\n",
      "1498 49434\n",
      "1499 49467\n",
      "1500 49500\n",
      "1501 49533\n",
      "1502 49566\n",
      "1503 49599\n",
      "1504 49632\n",
      "1505 49665\n",
      "1506 49698\n",
      "1507 49731\n",
      "1508 49764\n",
      "1509 49797\n",
      "1510 49830\n",
      "1511 49863\n",
      "1512 49896\n",
      "1513 49929\n",
      "1514 49962\n",
      "1515 49995\n",
      "1516 50028\n",
      "1517 50061\n",
      "1518 50094\n",
      "1519 50127\n",
      "1520 50160\n",
      "1521 50193\n",
      "1522 50226\n",
      "1523 50259\n",
      "1524 50292\n",
      "1525 50325\n",
      "1526 50358\n",
      "1527 50391\n",
      "1528 50424\n",
      "1529 50457\n",
      "1530 50490\n",
      "1531 50523\n",
      "1532 50556\n",
      "1533 50589\n",
      "1534 50622\n",
      "1535 50655\n",
      "1536 50688\n",
      "1537 50721\n",
      "1538 50754\n",
      "1539 50787\n",
      "1540 50820\n",
      "1541 50853\n",
      "1542 50886\n",
      "1543 50919\n",
      "1544 50952\n",
      "1545 50985\n",
      "1546 51018\n",
      "1547 51051\n",
      "1548 51084\n",
      "1549 51117\n",
      "1550 51150\n",
      "1551 51183\n",
      "1552 51216\n",
      "1553 51249\n",
      "1554 51282\n",
      "1555 51315\n",
      "1556 51348\n",
      "1557 51381\n",
      "1558 51414\n",
      "1559 51447\n",
      "1560 51480\n",
      "1561 51513\n",
      "1562 51546\n",
      "1563 51579\n",
      "1564 51612\n",
      "1565 51645\n",
      "1566 51678\n",
      "1567 51711\n",
      "1568 51744\n",
      "1569 51777\n",
      "1570 51810\n",
      "1571 51843\n",
      "1572 51876\n",
      "1573 51909\n",
      "1574 51942\n",
      "1575 51975\n",
      "1576 52008\n",
      "1577 52041\n",
      "1578 52074\n",
      "1579 52107\n",
      "1580 52140\n",
      "1581 52173\n",
      "1582 52206\n",
      "1583 52239\n",
      "1584 52272\n",
      "1585 52305\n",
      "1586 52338\n",
      "1587 52371\n",
      "1588 52404\n",
      "1589 52437\n",
      "1590 52470\n",
      "1591 52503\n",
      "1592 52536\n",
      "1593 52569\n",
      "1594 52602\n",
      "1595 52635\n",
      "1596 52668\n",
      "1597 52701\n",
      "1598 52734\n",
      "1599 52767\n",
      "1600 52800\n",
      "1601 52833\n",
      "1602 52866\n",
      "1603 52899\n",
      "1604 52932\n",
      "1605 52965\n",
      "1606 52998\n",
      "1607 53031\n",
      "1608 53064\n",
      "1609 53097\n",
      "1610 53130\n",
      "1611 53163\n",
      "1612 53196\n",
      "1613 53229\n",
      "1614 53262\n",
      "1615 53295\n",
      "1616 53328\n",
      "1617 53361\n",
      "1618 53394\n",
      "1619 53427\n",
      "1620 53460\n",
      "1621 53493\n",
      "1622 53526\n",
      "1623 53559\n",
      "1624 53592\n",
      "1625 53625\n",
      "1626 53658\n",
      "1627 53691\n",
      "1628 53724\n",
      "1629 53757\n",
      "1630 53790\n",
      "1631 53823\n",
      "1632 53856\n",
      "1633 53889\n",
      "1634 53922\n",
      "1635 53955\n",
      "1636 53988\n",
      "1637 54021\n",
      "1638 54054\n",
      "1639 54087\n",
      "1640 54120\n",
      "1641 54153\n",
      "1642 54186\n",
      "1643 54219\n",
      "1644 54252\n",
      "1645 54285\n",
      "1646 54318\n",
      "1647 54351\n",
      "1648 54384\n",
      "1649 54417\n",
      "1650 54450\n",
      "1651 54483\n"
     ]
    }
   ],
   "source": [
    "for batch, i in enumerate(range(0, 54496 - 1, 33)):\n",
    "    print(batch,i)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
